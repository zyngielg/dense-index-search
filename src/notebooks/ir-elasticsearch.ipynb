{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/gustaw/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/gustaw/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk \n",
    "import numpy as np\n",
    "import string\n",
    "import time\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from nltk.corpus import stopwords \n",
    "# reason for using snowball: https://stackoverflow.com/questions/10554052/what-are-the-major-differences-and-benefits-of-porter-and-lancaster-stemming-alg\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_data_path = '../../data/medqa/questions/US_qbank.jsonl'\n",
    "dev_questions_data_path = '../../data/medqa/questions/dev.jsonl'\n",
    "textbooks_data_dir = '../../data/medqa/textbooks/'\n",
    "\n",
    "questions_dev_medqa_path = '../../data/medqa/questions/4_options/dev.jsonl'\n",
    "questions_train_medqa_path ='../../data/medqa/questions/4_options/train.jsonl'\n",
    "questions_test_medqa_path ='../../data/medqa/questions/4_options/test.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_data(data, file_path):\n",
    "    with open(file_path, 'wb') as handle:\n",
    "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_pickle(file_path):\n",
    "    with open(file_path, 'rb') as handle:\n",
    "        return pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_dev = []\n",
    "questions_train = []\n",
    "questions_test = []\n",
    "\n",
    "with open(questions_dev_medqa_path, 'r') as file:\n",
    "    for line in file:\n",
    "        questions_dev.append(json.loads(line))\n",
    "\n",
    "with open(questions_train_medqa_path, 'r') as file:\n",
    "    for line in file:\n",
    "        questions_train.append(json.loads(line))\n",
    "        \n",
    "with open(questions_test_medqa_path, 'r') as file:\n",
    "    for line in file:\n",
    "        questions_test.append(json.loads(line))    \n",
    "\n",
    "corpus = {}\n",
    "for textbook_name in os.listdir(textbooks_data_dir):\n",
    "    textbook_path = textbooks_data_dir + '/' + textbook_name\n",
    "    with open(textbook_path, 'r') as textbook_file:\n",
    "        textbook_content = textbook_file.read()\n",
    "        corpus[textbook_name] = textbook_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "snowball_stemmer = SnowballStemmer(language='english') \n",
    "# do not remove the '-' and '/'\n",
    "custom_string_punctuation = string.punctuation.replace('-','').replace('/','').replace('.','')\n",
    "punctuation = str.maketrans('', '', custom_string_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_content(content, remove_stopwords, stemming, remove_punctuation):\n",
    "    if not remove_stopwords and not stemming and not remove_punctuation:\n",
    "        return content.lower().strip()\n",
    "    if remove_punctuation:\n",
    "        content = content.translate(punctuation).replace('“','').replace('’','')\n",
    "    sentences = nltk.sent_tokenize(content.lower().strip())\n",
    "    cleaned_sentences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence.lower())\n",
    "        if remove_stopwords:\n",
    "            tokens = [x for x in tokens if x not in stop_words]\n",
    "        if stemming:\n",
    "            tokens = [snowball_stemmer.stem(x) for x in tokens]\n",
    "        cleaned_sentences.append(' '.join(tokens))\n",
    "            \n",
    "    return ' '.join(cleaned_sentences)\n",
    "\n",
    "def preprocess_corpus(corpus, remove_stopwords, stemming, remove_punctuation):\n",
    "    for name, content in tqdm(corpus.items()):\n",
    "        # TODO: removal of non-medical terms using MetaMap\n",
    "        corpus[name] = preprocess_content(content, remove_stopwords, stemming, remove_punctuation)\n",
    "        \n",
    "        \n",
    "def preprocess_questions(questions, remove_stopwords, stemming, remove_punctuation, metamap=False):    \n",
    "    for question in tqdm(questions):\n",
    "        x = preprocess_content(question['question'], remove_stopwords, stemming, remove_punctuation)\n",
    "        question['question'] = x \n",
    "        for option, value in question['options'].items():\n",
    "            question['options'][option] = preprocess_content(value, remove_stopwords, stemming, remove_punctuation)\n",
    "        if metamap:\n",
    "            question['answer'] = preprocess_content(question['answer'], remove_stopwords, stemming, remove_punctuation)\n",
    "            for i, phrase in enumerate(question['metamap_phrases']):\n",
    "                question['metamap_phrases'][i] = preprocess_content(phrase, remove_stopwords, stemming, remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(remove_stopwords, stemming, remove_punctuation, metamap):\n",
    "    preprocess_corpus(\n",
    "        corpus=corpus,\n",
    "        remove_stopwords=remove_stopwords,\n",
    "        stemming=stemming,\n",
    "        remove_punctuation=remove_punctuation\n",
    "    )\n",
    "    preprocess_questions(\n",
    "        questions=questions_metamap_data,\n",
    "        remove_stopwords=remove_stopwords,\n",
    "        stemming=stemming,\n",
    "        remove_punctuation=remove_punctuation,\n",
    "        metamap=metamap\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1272/1272 [00:08<00:00, 153.90it/s]\n",
      "100%|██████████| 10178/10178 [01:06<00:00, 152.99it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocess_questions(\n",
    "    questions=questions_dev,\n",
    "    remove_stopwords=False,\n",
    "    stemming=True,\n",
    "    remove_punctuation=False,\n",
    "    metamap=True\n",
    ")\n",
    "\n",
    "preprocess_questions(\n",
    "    questions=questions_train,\n",
    "    remove_stopwords=False,\n",
    "    stemming=True,\n",
    "    remove_punctuation=False,\n",
    "    metamap=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5393c799a7044bf3b1bafe3ece3ab40b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=18.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preprocess_corpus(\n",
    "        corpus=corpus,\n",
    "        remove_stopwords=False,\n",
    "        stemming=True,\n",
    "        remove_punctuation=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus_chunks(chunk_length):\n",
    "    corpus_chunks = []\n",
    "    for title, content in tqdm(corpus.items()):\n",
    "\n",
    "        content_tokens = word_tokenize(content)\n",
    "\n",
    "        counter = 0\n",
    "        for i in range(0, len(content_tokens), chunk_length):\n",
    "            chunk_name = title + str(counter)\n",
    "            chunk = ' '.join(content_tokens[i:i+chunk_length])\n",
    "            chunk_processed = preprocess_content(chunk, False, False, False)\n",
    "            stemmed_chunk_processed = preprocess_content(chunk, False, True, False)\n",
    "            entry = {\n",
    "                'name': chunk_name,\n",
    "                'content': chunk_processed,\n",
    "                'content_stemmed': stemmed_chunk_processed\n",
    "            }\n",
    "            corpus_chunks.append(entry)\n",
    "            counter += 1\n",
    "    \n",
    "    return corpus_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb8a00527364431c9bee6533994b8de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=18.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'snowball_stemmer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-885fe8697372>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mchunk_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcorpus_chunks_100\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_corpus_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-89b410835cab>\u001b[0m in \u001b[0;36mcreate_corpus_chunks\u001b[0;34m(chunk_length)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mchunk_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mchunk_processed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mstemmed_chunk_processed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             entry = {\n\u001b[1;32m     14\u001b[0m                 \u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mchunk_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-823e90c6ff7c>\u001b[0m in \u001b[0;36mpreprocess_content\u001b[0;34m(content, remove_stopwords, stemming, remove_punctuation)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstemming\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msnowball_stemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mcleaned_sentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-823e90c6ff7c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstemming\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msnowball_stemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mcleaned_sentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'snowball_stemmer' is not defined"
     ]
    }
   ],
   "source": [
    "chunk_length = 100\n",
    "corpus_chunks_100 = create_corpus_chunks(chunk_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_chunks_50 = create_corpus_chunks(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus_sentences():\n",
    "    corpus_sentences = []\n",
    "    for title, content in tqdm(corpus.items()):\n",
    "        content_sentences = (nltk.sent_tokenize(content))\n",
    "        sentence_counter = 0\n",
    "        for sentence in content_sentences:\n",
    "            corpus_sentences.append({\n",
    "                'name': title + str(sentence_counter),\n",
    "                'content': sentence\n",
    "            })\n",
    "    return corpus_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_sentences = create_corpus_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch()\n",
    "\n",
    "def upload_documents(documents, index_name):\n",
    "    create_index_body = \"\"\"{\n",
    "        \"settings\": {\n",
    "            \"index\": {\n",
    "                \"similarity\": {\n",
    "                    \"default\": {\n",
    "                        \"type\": \"BM25\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\"\"\"\n",
    "    print(create_index_body)\n",
    "    es.indices.create(index=index_name, body=create_index_body)\n",
    "\n",
    "    id_counter = 1\n",
    "    for document in tqdm(documents):\n",
    "        res = es.index(index=index_name, id=id_counter, body=document)\n",
    "        id_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class Indexes(Enum):\n",
    "    Unprocessed_chunks_100 = \"unprocessed-chunks-100\",\n",
    "    Unprocessed_sentences = \"sentences-unprocessed-shards-1\",\n",
    "    Stemmed_sentences = \"sentences-stemmed-shards-1\"\n",
    "    MedQA_stemmed_chunks = \"medqa-stemmed-chunks\",\n",
    "    MedQA_unprocessed_chunks = \"medqa-unprocessed-chunks\",\n",
    "    MedQA_chunks_100 = \"medqa-chunks-100\"\n",
    "    MedQA_chunks_50 = \"medqa-chunks-50\"\n",
    "    #stemming-punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "        \"settings\": {\n",
      "            \"index\": {\n",
      "                \"similarity\": {\n",
      "                    \"default\": {\n",
      "                        \"type\": \"BM25\"\n",
      "                    }\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "    }\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87f738f55f064e4bbd1c687041aa5f47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=303096.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "upload_documents(corpus_chunks_100, Indexes.MedQA_chunks_100.value)\n",
    "upload_documents(corpus_chunks_50, Indexes.MedQA_chunks_50.value)\n",
    "upload_documents(corpus_sentences, Indexes.Unprocessed_sentences.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_documents(query_input, n, index_name):\n",
    "    res = es.search(\n",
    "        index=index_name, \n",
    "        body={\n",
    "            \"query\": {\n",
    "                \"match\": {\n",
    "                    \"content\": query_input\n",
    "                }\n",
    "            },\n",
    "            \"from\": 0,\n",
    "            \"size\": n\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    number_of_hits = len(res['hits']['hits'])\n",
    "    \n",
    "    results = []\n",
    "    for i in range(number_of_hits):\n",
    "        score = res['hits']['hits'][i]['_score']\n",
    "        paragraph = res['hits']['hits'][i]['_source']\n",
    "        result = {\n",
    "            \"score\": score,\n",
    "            \"evidence\": paragraph\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the collection of question: list of lists of documents retrieved per each option answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documents_from_elasticsearch(questions, num_of_docs, index):\n",
    "    retrieved_documents = {}\n",
    "    print(len(questions))\n",
    "    for idx, question_data in enumerate(tqdm(questions)):\n",
    "        question_documents = {}\n",
    "        question_id = \"q\" + str(idx)\n",
    "        for option, option_answer in question_data['options'].items():\n",
    "            query = ' '.join(question_data['metamap_phrases']) + ' ' + option_answer\n",
    "            top_documents = search_documents(query, num_of_docs, index)\n",
    "            question_documents[option_answer] = top_documents\n",
    "\n",
    "        retrieved_documents[question_id] = {\n",
    "            \"question\": question_data['question'], \n",
    "            \"retrieved_documents\": question_documents  \n",
    "        }\n",
    "    return retrieved_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f000aec369d41a38c14ced015d884cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1272.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d156cdf37843ada9e31db21466c97d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1272\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de870798051d4b5d9a69a54dbe5e3f8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1272.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10178\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "157bce6976f44c84adc3067e63106266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1272\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "791751384c2e493c9a138c1c37285983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1272.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10178\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e97c53efa1e346ccadfb6a0c5f9e31e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preprocess_questions(\n",
    "    questions=questions_dev,\n",
    "    remove_stopwords=False,\n",
    "    stemming=True,\n",
    "    remove_punctuation=False,\n",
    "    metamap=True\n",
    ")\n",
    "\n",
    "preprocess_questions(\n",
    "    questions=questions_train,\n",
    "    remove_stopwords=False,\n",
    "    stemming=True,\n",
    "    remove_punctuation=False,\n",
    "    metamap=True\n",
    ")\n",
    "\n",
    "preprocess_questions(\n",
    "    questions=questions_test,\n",
    "    remove_stopwords=False,\n",
    "    stemming=True,\n",
    "    remove_punctuation=False,\n",
    "    metamap=True\n",
    ")\n",
    "\n",
    "retrieved_documents_train = get_documents_from_elasticsearch(questions_train, 10, \"medqa-chunks-100-final\")\n",
    "retrieved_documents_val = get_documents_from_elasticsearch(questions_dev, 10, \"medqa-chunks-100-final\")\n",
    "retrieved_documents_test = get_documents_from_elasticsearch(questions_test, 10, \"medqa-chunks-100-final\")\n",
    "\n",
    "save_data(retrieved_documents_train_stemmed, \"final_es_retrieved_documents_train_chunks_100.pickle\")\n",
    "save_data(retrieved_documents_dev_stemmed, \"final_es_retrieved_documents_val_chunks_100.pickle\")\n",
    "save_data(retrieved_documents_test, \"final_es_retrieved_documents_test_chunks_100.pickle\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running IR-ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ir_es(questions, no_documents_to_retrieve, index_name, metamap=False, all_questions_bank=False):\n",
    "    start_time = time.time()\n",
    "\n",
    "    correct_answer = 0\n",
    "    incorrect_answer = 0\n",
    "    for question_data in tqdm(questions):\n",
    "        question = question_data['question']\n",
    "        # for all_questions, the answer is the letter\n",
    "        \n",
    "        if all_questions_bank:\n",
    "            answer = question_data['options'][question_data['answer']]\n",
    "        else:\n",
    "            answer = question_data['answer']\n",
    "\n",
    "        final_answer = None\n",
    "        final_score = 0\n",
    "\n",
    "        for option, option_answer in question_data['options'].items():\n",
    "            if metamap:\n",
    "                query = ' '.join(question_data['metamap_phrases']) + \" \" + option_answer\n",
    "            else:\n",
    "                query = question + \" \" + option_answer\n",
    "            top_documents = search_documents(query, no_documents_to_retrieve, index_name)\n",
    "            if top_documents != []:\n",
    "                score = 0\n",
    "                for doc in top_documents:\n",
    "                    score += doc['score']\n",
    "\n",
    "                if final_score < score:\n",
    "                    final_answer = option_answer\n",
    "                    final_score = score\n",
    "\n",
    "        correct = False\n",
    "        if final_answer == answer:\n",
    "            correct_answer += 1\n",
    "            correct = True\n",
    "        else:\n",
    "            incorrect_answer += 1\n",
    "\n",
    "\n",
    "    print(f'Accuracy: {100 * correct_answer / (correct_answer + incorrect_answer)}%')\n",
    "    print(f'\\tCorrect answers: {correct_answer}')\n",
    "    print(f'\\tInorrect answers: {incorrect_answer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ir_es([x], 5, 'unprocessed')\n",
    "def run_ir_es(questions, used_index, num_of_documents, metamap=False):\n",
    "    print(f'Used index: {used_index}\\nNumber of retrieved documents: {num_of_documents}\\nUsing metamap phrases: {metamap}')\n",
    "    ir_es(questions, num_of_documents, used_index, metamap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On stemmed index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used index: medqa-stemmed-chunks\n",
      "Number of retrieved documents: 10\n",
      "Using metamap phrases: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ceb66bb40c4a0e90b8cde528b4b6d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1272.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 27.90880503144654%\n",
      "\tCorrect answers: 355\n",
      "\tInorrect answers: 917\n"
     ]
    }
   ],
   "source": [
    "run_ir_es(questions=questions_dev,\n",
    "          used_index=Indexes.MedQA_stemmed_chunks.value,\n",
    "          num_of_documents=10,\n",
    "          metamap=True\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used index: medqa-stemmed-chunks\n",
      "Number of retrieved documents: 10\n",
      "Using metamap phrases: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a470beaa777e45f3bbab2d43f28e89c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 27.608567498526234%\n",
      "\tCorrect answers: 2810\n",
      "\tInorrect answers: 7368\n"
     ]
    }
   ],
   "source": [
    "run_ir_es(questions=questions_train,\n",
    "          used_index=Indexes.MedQA_stemmed_chunks.value,\n",
    "          num_of_documents=10,\n",
    "          metamap=True\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On unstemmed index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used index: medqa-unprocessed-chunks\n",
      "Number of retrieved documents: 10\n",
      "Using metamap phrases: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9540bcc65b446b1b7cb9fed3dcb6a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1272.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 31.761006289308177%\n",
      "\tCorrect answers: 404\n",
      "\tInorrect answers: 868\n"
     ]
    }
   ],
   "source": [
    "run_ir_es(questions=questions_dev,\n",
    "          used_index=Indexes.MedQA_unprocessed_chunks.value,\n",
    "          num_of_documents=10,\n",
    "          metamap=True\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used index: medqa-unprocessed-chunks\n",
      "Number of retrieved documents: 10\n",
      "Using metamap phrases: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebcae777053e4ec68823be56dbfe31e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 30.978581253684418%\n",
      "\tCorrect answers: 3153\n",
      "\tInorrect answers: 7025\n"
     ]
    }
   ],
   "source": [
    "run_ir_es(questions=questions_train,\n",
    "          used_index=Indexes.MedQA_unprocessed_chunks.value,\n",
    "          num_of_documents=10,\n",
    "          metamap=True\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ElasticSearch usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Neurology_Adams1', 'content': 'encounter . the originators of this book , raymond d. adams and maurice victor , insisted that the basis of the practice of neurology necessarily differs from that of neuroscience in that neurology is a medical discipline and must always be related back to the patient . here is the story : a 19-year-old college sophomore began to show paranoid traits . she became convinced that her roommate was listening in on her phone conversations and planning to alter her essays . she became reclusive and spent most of her time locked in her room . after much difficulty ,', 'content_stemmed': 'encount . the origin of this book , raymond d. adam and mauric victor , insist that the basi of the practic of neurolog necessarili differ from that of neurosci in that neurolog is a medic disciplin and must alway be relat back to the patient . here is the stori : a 19-year-old colleg sophomor began to show paranoid trait . she becam convinc that her roommat was listen in on her phone convers and plan to alter her essay . she becam reclus and spent most of her time lock in her room . after much difficulti ,'}\n"
     ]
    }
   ],
   "source": [
    "res = es.get(index=\"medqa-chunks-100-final\", id=2)\n",
    "print(res['_source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refreshing index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es.indices.refresh(index=\"unprocessed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching for a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = es.search(index=\"test-index\", body={\"query\": {\"match_all\": {}}})\n",
    "print(\"Got %d Hits:\" % res['hits']['total']['value'])\n",
    "for hit in res['hits']['hits']:\n",
    "    print(\"%(timestamp)s %(author)s: %(text)s\" % hit[\"_source\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting a document/index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch()\n",
    "\n",
    "# delete document\n",
    "# es.delete(index=\"test-index\", id=1)\n",
    "# delete index\n",
    "es.indices.delete(index=Indexes.MedQA_stemmed_chunks.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# curl \"localhost:9200/_cat/indices?v=true\"\n",
    "# curl -X GET \"localhost:9200/_cat/health?v=true&pretty\"\n",
    "# curl -X GET \"localhost:9200/sentences-stemmed/_settings\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}